---
title: Polynomial Regression
output:
  xaringan::moon_reader:
    css: "lab-slides.css"
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

### Kentucky Derby data

.large[
Data on the Kentucky Derby winner from each year between 1875 and 2012. Variables include

* `year`: year of race
* `speed`: speed of winning horse (mph)
* `time`: winning time (seconds)
* `condition`: condition of the track on the day of the race
]

---

### EDA

```{r echo=F, message=F, warning=F, fig.align='center', fig.width=7, fig.height=5}
library(tidyverse)
derby <- read.delim("~/Documents/Teaching/sta112-s22.github.io/slides/KYDerby17.txt")

derby %>%
  ggplot(aes(x = Year, y = Time)) +
  geom_point(size = 2.5) +
  geom_line() +
  theme_bw() +
  theme(text = element_text(size = 25))
```

.large[
.question[
What do you notice about the winning times?
]
]

--

.large[
There's a big drop around 1900, then they decrease gradually.
]

---

### EDA

```{r echo=F, message=F, warning=F, fig.align='center', fig.width=7, fig.height=5}

derby %>%
  ggplot(aes(x = Year, y = speed)) +
  geom_point(size = 2.5) +
  theme_bw() +
  theme(text = element_text(size = 25))
```

.large[
.question[
How would you describe the relationship?
]
]

--

.large[
There's a positive relationship, but it looks nonlinear.
]

---

### Fitting a linear regression

.large[
$$\text{Speed} = \beta_0 + \beta_1 \ \text{Year} + \varepsilon$$
]

```{r echo=F, message=F, warning=F, fig.align='center', fig.width=7, fig.height=5}

derby %>%
  ggplot(aes(x = Year, y = speed)) +
  geom_point(size = 2.5) +
  geom_smooth(se=F, method="lm") +
  theme_bw() +
  theme(text = element_text(size = 25))
```

.large[
.question[
How would I check whether the linear fit is appropriate?
]
]

---

### Residual plot

```{r echo=F, message=F, warning=F, fig.align='center', fig.width=7, fig.height=5}

derby_lm <- lm(speed ~ Year, data = derby)

derby %>%
  mutate(pred = predict(derby_lm),
         resid = residuals(derby_lm)) %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point(size = 2.5) +
  geom_abline(slope = 0, intercept = 0, color = "blue", lwd=1.2) +
  labs(x = "Predicted speed", y = "Residual") +
  theme_bw() +
  theme(text = element_text(size = 25))
```

.large[
.question[
Does the shape assumption look reasonable?
]
]

--

.large[
No -- there appears to be a nonlinear relationship
]

---

### Residual plot


```{r echo=F, message=F, warning=F, fig.align='center', fig.width=7, fig.height=5}

derby_lm <- lm(speed ~ Year, data = derby)

derby %>%
  mutate(pred = predict(derby_lm),
         resid = residuals(derby_lm)) %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point(size = 2.5) +
  geom_abline(slope = 0, intercept = 0, color = "blue", lwd=1.2) +
  labs(x = "Predicted speed", y = "Residual") +
  theme_bw() +
  theme(text = element_text(size = 25))
```

.large[
.question[
What should we do if the shape assumption doesn't look reasonable?
]
]

---

### Trying a transformation

.large[
$$\text{Speed} = \beta_0 + \beta_1 \log(\text{Year}) + \varepsilon$$
]

.pull-left[
```{r echo=F, message=F, warning=F, fig.align='center', fig.width=6.5, fig.height=4.5}

derby %>%
  ggplot(aes(x = Year, y = speed)) +
  geom_point(size = 2.5) +
  geom_smooth(se=F, method="lm", formula = y ~ log(x)) +
  theme_bw() +
  theme(text = element_text(size = 25))
```
]

.pull-right[
```{r echo=F, message=F, warning=F, fig.align='center', fig.width=6.5, fig.height=4.5}

derby_lm <- lm(speed ~ log(Year), data = derby)

derby %>%
  mutate(pred = predict(derby_lm),
         resid = residuals(derby_lm)) %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point(size = 2.5) +
  geom_abline(slope = 0, intercept = 0, color = "blue", lwd=1.2) +
  labs(x = "Predicted speed", y = "Residual") +
  theme_bw() +
  theme(text = element_text(size = 25))
```
]

.large[
.question[
Does the log transformation improve the fit?
]
]

---

### Polynomial transformations

.large[
Instead of using a log, what if we use a *polynomial*?

.question[
**Definition:** A polynomial transformation of order $k$ is a linear combination of the terms $X$, $X^2$, ..., $X^k$:
$$Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \cdots + \beta_k X^k + \varepsilon$$
]
]

.large[
* Example: If we use a polynomial of order 3, our model is 
$$Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \varepsilon$$
]

---

### Polynomial transformations

.large[
Instead of using a log, what if we use a *polynomial*?
]

```{r echo=F, message=F, warning=F, fig.align='center', fig.width=7, fig.height=5}

derby %>%
  ggplot(aes(x = Year, y = speed)) +
  geom_point(size = 2.5) +
  theme_bw() +
  theme(text = element_text(size = 25))
```

.large[
.question[
What order polynomial do you think we should use?
]
]

---

### Polynomial transformation

.large[
Let's try a polynomial of order 2:

$$\text{Speed} = \beta_0 + \beta_1 \ \text{Year} +  \beta_2 \ \text{Year}^2 + \varepsilon$$
]

```{r echo=F, message=F, warning=F, fig.align='center', fig.width=7, fig.height=5}

derby %>%
  ggplot(aes(x = Year, y = speed)) +
  geom_point(size = 2.5) +
  geom_smooth(se=F, method="lm", formula = y ~ I(x) + I(x^2), lwd=1.2) +
  theme_bw() +
  theme(text = element_text(size = 25))
```

---

### Diagnostic plots

.large[
We use residual and QQ plots to check the model assumptions after transforming our predictor, just like with other transformations.
]

```{r echo=F, message=F, warning=F, fig.align='center', fig.width=6, fig.height=4}

derby_lm <- lm(speed ~ I(Year) + I(Year^2), data = derby)

derby %>%
  mutate(pred = predict(derby_lm),
         resid = residuals(derby_lm)) %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point(size = 2.5) +
  geom_abline(slope = 0, intercept = 0, color = "blue", lwd=1.2) +
  labs(x = "Predicted speed", y = "Residual") +
  theme_bw() +
  theme(text = element_text(size = 25))
```

.large[
.question[
How does the shape assumption look now?
]
]

---

### Fitting the model in R

.large[
```{r eval=F}
derby_lm <- lm(speed ~ I(Year) + I(Year^2), 
               data = derby)
```
]

.large[
* `I(...)` is used in R when we want to evaluate the expression arithmetically.
* `^` is used to raise a number to a power. E.g. `Year^2` means $\text{Year}^2$, `Year^3` means $\text{Year}^3$, etc.
]

---

### Fitting the model in R

.large[
```{r}
derby_lm <- lm(speed ~ I(Year) + I(Year^2), 
               data = derby)
derby_lm
```
]

.large[
$$\widehat{\text{Speed}} = -971.8 + 1.013 \ \text{Year} - 0.00025 \ \text{Year}^2$$
]

---

### Interpreting the fitted model

.large[
$$\widehat{\text{Speed}} = -971.8 + 1.013 \ \text{Year} - 0.00025 \ \text{Year}^2$$
]

.large[
.question[
How would I interpret the intercept of the fitted polynomial regression model?
]
]

--

.large[
If Year = 0, we predict a speed of -971.8 mph (doesn't make much sense, since year is nowhere near 0)
]

---

### Interpreting the fitted model

.large[
$$\widehat{\text{Speed}} = -971.8 + 1.013 \ \text{Year} - 0.00025 \ \text{Year}^2$$
]

.large[
.question[
How would I interpret the terms on $\text{Year}$ and $\text{Year}^2$?
]
]

--

.large[
They're kind of difficult to interpret -- if I change $\text{Year}$, I also change $\text{Year}^2$
]

--

.large[
.question[
Can I interpret the *sign* on $\text{Year}^2$?
]
]

--

.large[
Yes (for a polynomial of order 2 specifically). The sign on $\text{Year}^2$ tells us about the concavity of the relationship.
]

---

### Class activity

.large[
[https://sta112-s22.github.io/class_activities/ca_lecture_15.html](https://sta112-s22.github.io/class_activities/ca_lecture_15.html)
]

---

### Class activity

.large[
.question[
What is the equation of the fitted model, using a polynomial of order 2?
]
]

--

.large[
$$\widehat{\text{CO2}} = 414.975 - 0.476 \ \text{Day} + 0.0012 \ \text{Day}^2$$
]

---

### Class activity

.large[
$$\widehat{\text{CO2}} = 414.975 - 0.476 \ \text{Day} + 0.0012 \ \text{Day}^2$$
]

.large[
.question[
Based on the fitted model, is the relationship between Day and CO2 concave up or concave down?
]
]

--

.large[
Concave up, since the coefficient for $\text{Day}^2$ is positive.
]

---

### Class activity

.large[
$$\widehat{\text{CO2}} = 414.975 - 0.476 \ \text{Day} + 0.0012 \ \text{Day}^2$$
]

.large[
.question[
What is the predicted CO2 level for day 200?
]
]

--

.large[
$414.975 - 0.476 \cdot 200 + 0.0012 \cdot 200^2 = 367.775$ ppm
]

---

### Class activity

.large[
.question[
Does a polynomial of order 3 look like it fits better?
]
]

--

```{r echo=F, message=F, warning=F, fig.align='center', fig.width=7, fig.height=5}

library(Stat2Data)
data("CO2Germany")

CO2Germany %>%
  ggplot(aes(x = Day, y = CO2)) +
  geom_point(size = 2.5) +
  geom_smooth(se=F, method="lm", formula = y ~ I(x) + I(x^2), lwd=1.2) +
  geom_smooth(se=F, method="lm", formula = y ~ I(x) + I(x^2) + I(x^3), lwd=1.2,
              color = "red", lty=2) +
  theme_bw() +
  theme(text = element_text(size = 25))
```

.large[
No, the fits look very similar
]

---

### Measuring regression fit

```{r echo=F, message=F, warning=F, fig.align='center', fig.width=7, fig.height=4}

CO2Germany %>%
  ggplot(aes(x = Day, y = CO2)) +
  geom_point(size = 2.5) +
  theme_bw() +
  theme(text = element_text(size = 25))
```

.large[
.question[
How do I measure strength of the linear relationship between two quantitative variables?
]
]

--

.large[
The correlation
]

---

### Measuring regression fit

```{r echo=F, message=F, warning=F, fig.align='center', fig.width=7, fig.height=4}

CO2Germany %>%
  ggplot(aes(x = Day, y = CO2)) +
  geom_point(size = 2.5) +
  theme_bw() +
  theme(text = element_text(size = 25))
```

.large[
.question[
Is the correlation appropriate when the relationship is nonlinear?
]
]

--

.large[
Not really -- correlation deals with *linear* relationships
]

---

### Measuring variability in the response

.large[
* When we fit a regression model with least squares, we choose parameters to minimize the sum of squared residuals:

$$SSE = \sum_i (y_i - \widehat{y}_i)^2$$
]

.large[
* We can think of SSE as the "amount of variability" in $Y$ after predicting with our regression model
* The amount of variability in $Y$ *before* predicting with our regression model is

$$SSTotal = \sum_i (y_i -\overline{y})^2$$
]

---

### $R^2$: the coefficient of determination

.large[
$$SSE = \sum_i (y_i - \widehat{y}_i)^2$$
$$SSTotal = \sum_i (y_i -\overline{y})^2$$
]

.large[
The proportion of variability in $Y$ "explained" by our regression model is 

$$R^2 = 1 - \dfrac{SSE}{SSTotal}$$
]

.large[
* $R^2$ is between 0 and 1
* A higher $R^2$ means our model is a "better fit" to the data
* For regression with one variable, $R^2 = r^2$ (the squared correlation)
]

---

### $R^2$

.large[
```{r}
co2_lm <- lm(CO2 ~ I(Day) + I(Day^2), 
               data = CO2Germany)
summary(co2_lm)$r.squared
```
]

.large[
$R^2 = 0.573$, so we explain about 57% of the variability in CO2 levels with a polynomial regression of order 2 on Day.
]

---

### Comparing $R^2$

.large[
```{r}
co2_lm <- lm(CO2 ~ I(Day) + I(Day^2), 
               data = CO2Germany)
summary(co2_lm)$r.squared
```
]

.large[
```{r}
co2_lm <- lm(CO2 ~ I(Day) + I(Day^2) + I(Day^3), 
               data = CO2Germany)
summary(co2_lm)$r.squared
```
]

.large[
.question[
$R^2$ has increased very slightly -- does that mean that a polynomial of order 3 is a better fit?
]
]

---

### $R^2$

.large[
$R^2$ **will always increase when you add more parameters to the model.**

Why? Because

$$R^2 = 1 - \dfrac{SSE}{SSTotal}$$
and adding more parameters gives us more flexibility to decrease SSE.
]

---

### Adjusted $R^2$

.large[
We can account for the number of parameters in the model with *adjusted* $R^2$:

$$R^2_{adj} = 1 - \dfrac{SSE/(n-p)}{SSTotal/(n-1)}$$
]

.large[
* $n =$ number of observations in data (sample size)
* $p =$ number of parameters ( $\beta$ terms) in model
]

---

### $R^2_{adj}$

.large[
```{r}
co2_lm <- lm(CO2 ~ I(Day) + I(Day^2), 
               data = CO2Germany)
summary(co2_lm)$adj.r.squared
```
]

.large[
```{r}
co2_lm <- lm(CO2 ~ I(Day) + I(Day^2) + I(Day^3), 
               data = CO2Germany)
summary(co2_lm)$adj.r.squared
```
]

.large[
We can see that $R^2_{adj}$ is slightly higher for the polynomial of order 2
]

---

### Properties of $R^2_{adj}$

.large[
* $R^2_{adj} < R^2$ (always)
* $R^2_{adj}$ accounts for the number of parameters in the model
* $R^2$ can *never* decrease when predictors are added
* $R^2_{adj}$ *can* decrease if unhelpful predictors are added
]